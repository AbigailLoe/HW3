---
title: "vignettetest"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignettetest}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(myHW3)
```

# The data to input:
The data should be in the form of a simple linear regression, with all covariates you wish to regress upon should be included in the data.

For example, say my data ahs an $X_1, X_2, X_3$ covariate.

One also needs to input the observed responses in a $Y$ parameter.

First, some generated covariate values.
```{r}
X = matrix(rnorm(666), ncol = 3)
Y = sample(1:nrow(X)) +rnorm(nrow(X), mean = 0, sd = .2)
```


If I want to get the linear regression $E[Y] = \beta_0+\beta_1X_1+ \beta_2X_2+ \beta_3X_3$, then I run:
```{r}
model1 = linearModel(X, Y)
```

To test this compared to the base R command, first inpute the data in the correct form.

```{r}
dat = cbind.data.frame(X, Y)
names(dat) = c("x1", "x2", "x3", "y")
test1 = lm(y~., data = dat)
```

Now compare the results:

```{r}
summary(test1)
model1
```
Just even looking at the results, they are very similar. Coefficient estimates are almost identical. There is some slight deviation in the t-values which results in minor deviations in the p-values, but I chalk that up to a rounding error/rounding differnece in code.


# Equivalence Testing
To test the equivalence of the p-value estimates:
```{r}
lmPval = summary(test1)$coefficients[,4]
names(lmPval) = NULL
all.equal(unlist(round(model1["p-value"], 3), use.names = FALSE),
          round(lmPval, 3))
```

To test the equivalence of the coefficient estimates:
```{r}
lmEst = summary(test1)$coefficients[,1]
names(lmEst) = NULL
all.equal(unlist(round(model1["Estimate"], 5), use.names = FALSE),
          round(lmEst, 5))
```

To test the equivalence of the t-values:

```{r}
lmT = summary(test1)$coefficients[,3]
names(lmT) = NULL
all.equal(unlist(round(model1["t-value"], 3), use.names = FALSE),
          round(lmT, 3))
```


To test standard deviation of the coefficient estimates:
```{r}
lmSHat = summary(test1)$coefficients[,2]
names(lmSHat) = NULL
all.equal(unlist(round(model1["Std. Error"], 5), use.names = FALSE),
          round(lmSHat, 5))
```

# A more complicated model.
If one wishes to build a more complicated regression, you simply need ot input the correct design matrix. If my model is $E[Y] = \beta_0 +\beta_1 X_1 + \beta_2 X_2 +\beta_3 X_1\times X_2$, then:

```{r}
X2 = cbind(X[,1:2], X[,1]*X[,2])
model2 = linearModel(X2, Y)
test2 = lm(Y~x1*x2, data = dat)
summary(test2)$coefficients
model2
```

# Possible Errors
If an overfit regression is attempted, `linearModel` will throw an error.
```{r}
X = matrix(rnorm(42), ncol = 6, nrow = 7)
Y= rnorm(7)
linearModel(X,Y)
```

